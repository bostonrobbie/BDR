# Memory

## Me
Rob Gorham, BDR at Testsigma. I reach out to QA/testing leaders to book meetings and drive pipeline for our agentic AI test automation platform.

## Company
**Testsigma** — Agentic AI-powered unified test automation platform. Write tests in plain English, AI creates/runs/heals them. Web, mobile, API, desktop, Salesforce, SAP. Founded 2019, HQ San Francisco, ~196 employees. $12.8M total funding (Series A led by MassMutual Ventures).

## People
| Who | Role |
|-----|------|
| **Rukmangada Kandyala** | CEO & Founder, Testsigma |
| **Pratheep Velicherla** | Co-founder |
| **Rajesh Reddy** | Co-founder |
| **Vikram Chaitanya** | Co-founder |

## Key Product Terms
| Term | Meaning |
|------|---------|
| **Atto** | AI coworker — suite of agents (Generator, Sprint Planner, Runner, Analyzer, Healer, Optimizer) |
| **Atto 2.0** | Nov 2025 release — intent-based self-healing, coverage discovery, risk analysis |
| **NLP** | Natural Language Programming — write tests in plain English |
| **Copilot** | GenAI assistant that generates tests from prompts, Figma, Jira, screenshots |
| **Self-healing** | AI auto-fixes broken locators/tests when UI changes (90% maintenance reduction) |

## Core Value Props (3 Pain Hooks)
1. **Flaky/brittle tests** → AI self-healing. Story: Hansard cut regression 8→5 weeks
2. **Too much time creating/running tests** → Plain English + parallel execution. Story: Medibuddy 2,500 tests, 50% maintenance cut
3. **Can't scale coverage** → NLP + AI agent + cross-browser. Story: CRED 90% regression coverage, 5x faster

## Target Personas
| Title | Priority | Why |
|-------|----------|-----|
| QA Manager / QA Lead | Primary | Feels pain daily, authority to evaluate |
| Director/VP of QA | Primary | Strategic, has budget |
| Software Eng Manager | Secondary | Owns QA at companies w/o dedicated QA dept |
| VP Engineering / CTO | Secondary | Budget holder, strategic champion |
| Senior SDET / Automation Lead | Influencer | Technical champion, validates in trial |

## Top Verticals
SaaS/Tech, FinTech, Retail/E-Commerce, Healthcare/Digital Health, Telecom, Pharma

## Key Customers
Cisco, Samsung, Honeywell, Bosch, Nokia, Nestle, KFC, DHL, Zeiss, Axel Springer, NTUC Fairprice, Oscar Health, Sanofi, Spendflo, Nagra DTV, American Psychological Association

## Outreach Data
- Email reply rate ~1% overall; QA Engineers best (~1.4%), VPs lowest (~0.5%)
- Replies often come on email 3-5 in a sequence
- Best West Coast calling: 8-11 AM PT and 3-6 PM PT
- Top BDRs do 30+ touches per account, multi-channel

## Preferences
- Conversational, consultative BDR style (not scripted)
- BANT + Techstack discovery framework
- Use customer stories matched to prospect pain
- Keep emails short (<150 words), one CTA, social proof

---

## Outreach SOP (for Claude)

This section tells Claude how to build prospect lists and write outreach messages for Rob. Follow these rules exactly.

### Writing Style Rules
- **NO em dashes (—).** Use commas or short hyphens (-) only. Prefer commas.
- Must sound like Rob wrote it personally. Not AI-generated, not templated, not formulaic.
- Warm, conversational, low-pressure, specific, respectful.
- No feature dumping. Focus on a specific solution to something the prospect is already doing.
- Mobile-friendly: 75-95 words for Touch 1. Max 6-8 short lines with spacing.
- Clear spacing between paragraphs. No wall of text.
- **Never "show your work."** Do NOT reference LinkedIn, profiles, resume facts, or years at a company. The research happens in the backend. The message synthesizes it into a sharp insight.
- No "I noticed" or "I saw" or "based on your profile" or resume recaps.
- No generic assertions like "you must be doing manual testing." Ask, don't assert.
- Reading level: simple words, short sentences, no corporate phrasing or buzzwords.
- At least one real question mark in the message (preferably two: one early, one at the close).
- The close must have a question mark, must directly tie to the value angle above it, and must frame the meeting as a benefit to them.
- **No easy-out lines.** Do not say "no worries," "if not, all good," "feel free to ignore," or any variant. Just ask for the meeting confidently and casually.
- Do not frame anything as benefiting us. No "I want to show you" or "I'd love to share."

### Message Structure (C1 Style)
The C1 style synthesizes all research into a natural message that feels like one clear thought, not a formula with stitched-together parts. The 5 elements below must be present but invisible as structure.

1. **Subject line** - Short (2-4 words), specific enough to open, not clickbaity. "Quick question" works. So does a 2-word reference to their domain.
2. **Opening question** (1-2 lines) - A curiosity-led question that connects to a plausible initiative or challenge at their company. Do NOT cite LinkedIn. Synthesize the research into an insight. Phrased as "Is [specific thing] keeping pace with [specific growth/change]?" This should be something only someone who understands their world would ask.
3. **Context sentence** (1-2 lines) - One sentence that explains WHY the question matters, grounded in what typically happens for teams like theirs. This builds credibility without being generic. Pattern: "That's the thing that usually [specific consequence] for [specific type of team]."
4. **Proof point** (1-2 lines) - One verified customer example with real numbers, matched to their vertical and pain. Do not invent stats. Frame it as what the customer achieved, not what Testsigma does. Mention Testsigma by name once here.
5. **Close** (1 line) - A confident, casual meeting ask with a question mark. Must directly reference the value angle set up above it (e.g., if the message is about maintenance, the close asks about maintenance). Frame as "Would [specific time] to [specific action tied to the proof point] be useful for [what they're dealing with]?" No easy outs, no "no worries."

**What the message must NOT sound like:**
- "Hey [Name], I saw you worked at [Company] for [X] years leading [team]. That must mean [assumption]. [Client] saved [number]. Would 15 minutes make sense?"
- That formula is dead. Every prospect has seen it 100 times. C1 messages must feel like one coherent thought from someone who understands their world, not a template with variables filled in.

### Multi-Channel Sequence (write all touches per prospect)
Every prospect gets a multi-channel sequence. All written touches follow the same writing style rules. Rob decides exact timing/spacing manually.

**Day 1 - Touch 1 (InMail):** Full message with all 6 elements above. 70-120 words.
**Day 3 - Touch 2 (Cold Call):** Use the personalized call snippet (see Cold Call Prep below). 30-second opener max.
**Day 5 - Touch 3 (InMail Follow-up):** Shorter, 40-70 words. New angle or new proof point, NOT a rehash of Touch 1. Reference that you reached out before but keep it light ("Circling back quick..."). Add one new piece of value: a different customer story, a specific capability match, or a recent company event. End with a softer ask ("Worth a conversation?" or "Happy to share more if helpful.").
**Day 8 - Touch 4 (Cold Call #2):** Different angle than Call #1. Lead with the proof point from Touch 3.
**Day 10 - Touch 5 (Email):** Short email (if email address is available). Same rules as InMail but can be slightly more direct since email feels less intrusive.
**Day 15 - Touch 6 (InMail Break-up):** Shortest, 30-50 words. Acknowledge the silence without guilt-tripping. Offer to close the loop: "If the timing isn't right, totally get it. Just wanted to close the loop so I'm not clogging your inbox." Leave the door open but make it easy to say no.

**Sequence rules:**
- Each written touch must use a DIFFERENT proof point or angle than the previous touch.
- Touches 3-6 do NOT need the full 6-element structure. They can be looser.
- Touch 6 should never pitch. It's purely a respectful close-out.
- If a prospect has Buyer Intent, Touch 3 can be more direct ("Noticed your team's been exploring options in this space...").
- Not every prospect will have email addresses. If no email, skip Touch 5 and adjust spacing.
- Rob manually executes calls. Claude provides the call script snippets, Rob makes the dial.

### Cold Call Prep (per prospect)
Every prospect card in the deliverable includes a personalized cold call snippet. This is NOT a full script, it's a 3-line cheat sheet Rob can glance at before dialing.

**Call snippet structure:**
1. **Opener** (1 line): "Hey [Name], this is Rob from Testsigma - [personalized hook referencing their role or company]."
2. **Pain hypothesis** (1 line): "[Specific testing problem tied to their company context]."
3. **Bridge to ask** (1 line): "We helped [proof point]. Worth 60 seconds to see if it's relevant?"

**Rules:**
- Total call snippet: 3 lines max. Rob needs to glance and dial, not read a paragraph.
- The opener must reference something specific enough that it doesn't sound like a cold call script.
- The pain hypothesis should be different from what was used in Touch 1 InMail (variety across channels).
- Use a different proof point than Touch 1 to avoid sounding repetitive if they read the InMail.

### Objection Pre-Mapping (per prospect)
Based on company research, predict the most likely objection for each prospect and pre-load the response in the tracker.

**Common objection triggers (detect during research):**
| Research Signal | Likely Objection | Pre-loaded Response |
|----------------|-----------------|-------------------|
| Uses TOSCA, Katalon, Testim, or mabl | "We already have a tool" | "Totally fair. A lot of teams we work with had [tool] too. The gap they kept hitting was [specific limitation]. Worth comparing?" |
| 50K+ employee company | "Security/procurement is complex" | "We offer on-prem, private cloud, and hybrid. SOC2/ISO certified. A few Fortune 500s run us behind their firewall." |
| No dedicated QA team visible | "QA isn't a priority" | "That's actually why teams like yours use us. Plain English means devs write tests without a dedicated QA team." |
| Recently hired QA leader | "Too early, still assessing" | "Makes sense. A lot of QA leaders in their first 90 days use our free trial to benchmark what's possible before committing." |
| Pharma/healthcare/finance | "Compliance requirements" | "We work with Sanofi, Oscar Health, and several banks. Happy to walk through our compliance story." |
| Startup/small team (<200 employees) | "Budget is tight" | "Totally get it. One company your size (Spendflo) cut manual testing 50% and saw ROI in the first quarter." |

**Rules:**
- Assign ONE predicted objection per prospect (the most likely one based on research).
- Include the pre-loaded response in the tracker as a tooltip or expandable field.
- If no clear signal, default to "We already have a tool" since that's the most common objection.

### Customer Proof Points (use these, match to pain)
| Proof Point | Best For |
|-------------|----------|
| Hansard: regression 8 weeks → 5 weeks with AI auto-heal | Insurance, financial services, long regression cycles |
| Medibuddy: 2,500 tests automated, 50% maintenance cut | Healthcare, mid-size teams, scaling coverage |
| CRED: 90% regression automation, 5x faster execution | FinTech, high-velocity teams |
| Sanofi: regression 3 days → 80 minutes | Pharma, compliance-heavy, healthcare |
| Fortune 100: 3X productivity increase | Enterprise, VP-level, big tech |
| Nagra DTV: 2,500 tests in 8 months, 4X faster | Media, streaming, API + UI testing |
| Spendflo: 50% manual testing cut | SaaS, smaller teams, quick wins |
| 70% maintenance reduction vs Selenium | Teams on Selenium/Cypress/Playwright |
| 90% maintenance reduction with self-healing | Anyone complaining about flaky/brittle tests |

### Research Requirements (TWO sources per prospect)
1. **LinkedIn profile** - Read headline, about section, role description, responsibilities, recent activity. Capture and log the Sales Navigator profile URL.
2. **Company research** - From credible external sources (company website, product pages, news, engineering blog, job postings, press releases). NOT just the LinkedIn company page.

### Sales Navigator Workflow
1. Use Rob's saved searches in Sales Navigator.
2. Click "Show X new results" to filter to fresh, never-contacted prospects only.
3. Manager level and above only. Must fit ICP titles (see Target Personas above).
4. **MUST check interaction status** on each profile. If "Messaged:" or "Viewed:" shows prior activity, EXCLUDE and replace.
5. Filter out: pharma/biotech manufacturing QA (not software), non-US, titles that don't own software testing decisions.
6. Extract data using bulk JavaScript DOM extraction from search pages, then visit individual profiles for deep research.
7. Use `get_page_text` for profile data extraction (more reliable than screenshots).
8. Launch parallel Task subagents for company research to save time.

### Prospect Mix Ratio
Aim for this composition in every 25-prospect batch:
- **12-15 QA-titled leaders** (Director of QA, Head of QA, VP Quality Engineering, Sr Director Quality Engineering, QA Manager). These are the highest-reply-rate personas (~1.0-1.4%) and feel the pain directly.
- **8-10 VP Engineering / VP SE** at relevant companies. These are budget holders but lower reply rate (~0.5-0.8%). Only include if: company is in a target vertical, company size is right (not 50K+ employee companies unless Buyer Intent), and their scope plausibly includes QA decisions.
- **2-3 Buyer Intent prospects** regardless of title. These are hot and should be prioritized.
- Within these, aim for vertical diversity: no more than ~8 from the same vertical in a single batch.

### Deliverable Format
- Single HTML file with:
  - Prospect tracker table sorted by priority score (descending), with columns: priority score, name, title, company, tags, profile research notes, company research notes, outreach angle, LinkedIn URL, status, reply tag, A/B group, personalization score
  - Individual prospect cards containing:
    - Copy-paste-ready messages for ALL written touches (Touch 1 InMail, Touch 3 Follow-up, Touch 5 Email if available, Touch 6 Break-up) with "Copy Message" and "Copy Subject" buttons
    - Cold call snippet for Touch 2 and Touch 4 (3-line cheat sheet with "Copy Script" button)
    - Predicted objection + pre-loaded response (expandable or tooltip)
    - Meeting prep card (auto-populated from research when status = Meeting Booked)
  - Status dropdown per prospect: Not Started, Touch 1 Sent, Call 1 Made, Touch 3 Sent, Call 2 Made, Touch 5 Sent, Touch 6 Sent, Replied, Meeting Booked, Not Interested, Bounced, Dormant, Re-Engaged
  - Reply tag dropdown per prospect (see Reply Tagging below)
  - A/B group label per prospect (see A/B Testing below)
  - Message personalization score (1-3) per prospect (see Message Scoring below)
  - Priority score badge (1-5) per prospect (see Priority Scoring below)
- Color-coded badges for persona type (QA, VP Eng), vertical (FinServ, Tech), and signals (Buyer Intent, Recently Hired)
- Priority filter: ability to show only Hot (5) and Warm (4) prospects
- Save to Work folder so Rob can open it directly.
- **Filename convention:** `prospect-outreach-[batch#]-[date].html` (e.g., `prospect-outreach-3-2026-02-18.html`)

### Reply Tagging
When a prospect replies (positive, negative, or referral), Rob tags WHAT triggered the reply using a dropdown:
- **Opener** - They referenced or reacted to the personalized opener
- **Pain hook** - They engaged with the problem hypothesis ("yeah, that's exactly our issue")
- **Proof point** - They asked about the customer story or numbers
- **Timing** - They said "good timing" or "we're evaluating"
- **Referral** - They forwarded to someone else or said "talk to [name]"
- **Not interested** - Replied but declined (still useful data)
- **Unknown** - Can't tell what triggered it

Even negative replies are data. "Not interested" with a reason tells us the pain hook missed. "Talk to [name]" tells us we hit the right company but wrong person.

### A/B Testing Within Batches
Split each 25-prospect batch into 2-3 sub-groups to test ONE variable at a time. Label each prospect with their A/B group in the tracker.

**What to A/B test (one per batch, rotate across batches):**
1. **Pain hook** - Group A gets "maintenance/flaky tests" angle, Group B gets "release velocity/speed" angle
2. **Proof point style** - Group A gets named customer (Sanofi, CRED), Group B gets anonymous ("a Fortune 100 company")
3. **Opener style** - Group A gets career-reference openers, Group B gets company-metric openers
4. **Ask intensity** - Group A gets "Would 15 minutes make sense?", Group B gets "Happy to share more if helpful"
5. **Message length** - Group A gets 70-80 words (tight), Group B gets 100-120 words (fuller)

**Rules:**
- Only test ONE variable per batch. Keep everything else constant across groups.
- Split groups evenly by persona type and vertical so results aren't skewed.
- Need 3+ batches testing the same variable to draw conclusions (small sample sizes lie).
- Track the A/B variable and group in the HTML tracker for each prospect.

### Message Scoring Rubric
Before sending, rate each message 1-3 on personalization depth:

| Score | Meaning | Example |
|-------|---------|---------|
| 3 | **Deep** - References something only THIS person would recognize (specific project, career move, years at company, unique responsibility) | "The 'AI Quality Initiatives' part of your title stood out" |
| 2 | **Medium** - References their company specifically but the opener could apply to anyone in that role there | "Directing QA at Ally's digital banking platform" |
| 1 | **Light** - Opener is mostly about the company/industry, not the person | "QA teams in fintech often tell us..." |

Track the score per prospect. Over time, compare reply rates by score level. Hypothesis: Score 3 messages reply at 2-3x the rate of Score 1.

### Feedback Loop & Batch Learning
- Each deliverable HTML includes status tracker, reply tags, A/B group labels, and personalization scores.
- Rob updates these as he sends messages and gets replies.
- Before building the NEXT batch, Claude reads ALL previous batch files and generates a **Pre-Brief** (see below).
- Claude uses the accumulated data to adjust:
  - Which proof points are getting replies (use more of what works)
  - Which opener styles are getting replies
  - Which persona types are converting
  - Which verticals are responsive
  - Which A/B test variants are winning
  - Whether higher personalization scores correlate with more replies
- Over time this creates a compounding advantage, each batch gets smarter than the last.
- If no previous batch file exists, start fresh with default assumptions from this SOP.

### Pre-Brief (generated before each new batch)
Before building a new batch, Claude reads all previous batch files and generates a 5-line "What's Working" summary:

1. **Best persona** - Which title/level is replying most? (e.g., "Directors of QA are replying at 3x the rate of VP Eng")
2. **Best proof point** - Which customer story is in the most replied-to messages? (e.g., "Sanofi 3-day-to-80-min resonates with compliance-heavy prospects")
3. **Best vertical** - Which industry is warmest? (e.g., "FinServ is 2x warmer than SaaS")
4. **Best pattern** - Any opener/ask/length pattern standing out? (e.g., "Career-reference openers outperform company-metric openers")
5. **Stop doing** - One thing to drop or change (e.g., "VP Eng at 50K+ companies: 0 replies across 3 batches, stop including")

This pre-brief goes at the top of the new batch HTML file so Rob can see the reasoning.

### Batch Comparison Dashboard
After 3+ batches exist, Claude can generate a standalone `outreach-dashboard.html` file that aggregates data across all batches:
- Reply rate by persona type (bar chart)
- Reply rate by vertical (bar chart)
- Reply rate by proof point used (bar chart)
- Reply rate by personalization score (bar chart)
- A/B test results (comparison table)
- Trend over time (line chart showing improvement across batches)
- Top 5 best-performing messages (full text, for pattern analysis)

Rob can request this dashboard at any time. It reads from all `prospect-outreach-*.html` files in the Work folder.

### Priority Scoring (per prospect)
Each prospect gets a priority score (1-5) that determines the order Rob works them. The tracker should be sorted by priority score descending.

**Scoring formula:**
| Factor | Points |
|--------|--------|
| Buyer Intent signal in Sales Navigator | +2 |
| QA-titled leader (Director/Head/VP of QA) | +1 |
| Company in top vertical (FinTech, SaaS, Healthcare) | +1 |
| Recently hired in role (<6 months) | +1 |
| Company in active digital transformation or migration (from research) | +1 |
| Company uses a known competitor tool (TOSCA, Katalon, Selenium, etc.) | +1 |
| VP Eng at 50K+ company (no QA-specific scope) | -1 |

**Priority tiers:**
- **5 (Hot):** Work first every morning. Call + InMail same day.
- **4 (Warm):** High priority. Start sequence within 24 hours.
- **3 (Standard):** Normal cadence. Start sequence within the batch cycle.
- **2 (Lower):** Fill out the batch but don't expect high conversion.
- **1 (Long shot):** Only include if batch needs volume. Skip if better options exist.

**In the deliverable:** Show the priority score prominently next to each prospect name. Sort the tracker table by priority descending. Add a filter to show only Hot/Warm prospects.

### Re-Engagement Triggers
After Touch 6 (break-up), a prospect goes dormant. But they should be re-engaged if any of these signals appear:

| Trigger | Action |
|---------|--------|
| **Buyer Intent reactivates** in Sales Navigator | New sequence with fresh angle. Reference the signal: "Noticed your team's been researching..." |
| **New QA job posting** at their company | New outreach: "Saw [Company] is hiring for QA - figured testing might be top of mind." |
| **Leadership change** (new CTO, VP Eng, or QA Director joins) | Reach out to the NEW person, not the old contact. Fresh start. |
| **Company raises funding** | New outreach: "Congrats on the round - scaling usually means scaling QA too." |
| **Company ships major product/feature** (from news) | New outreach: "Saw the launch of [product]. Curious if the testing effort behind it was painful." |
| **Testsigma ships a major feature** (e.g., new integration, Atto update) | Re-engage with the new capability as the hook. "Since we last talked, we launched [X]." |

**Rules:**
- Minimum 60 days between break-up and re-engagement (don't pester).
- Re-engagement messages must have a NEW reason for reaching out. Never just repeat the old sequence.
- Log re-engagements in the tracker with a "Re-engaged" status and the trigger reason.

### Meeting Booked Handoff
When a prospect says yes to a meeting, Rob needs to prep fast. The deliverable includes a "Prep" section per prospect that auto-generates when status changes to "Meeting Booked."

**Prep card contents:**
1. **Company snapshot** (from research): What they do, how big, key products, recent news.
2. **Prospect snapshot**: Title, tenure, responsibilities, career background, anything from their LinkedIn about/activity.
3. **Known/likely tech stack**: Any tools mentioned in their profile, job postings, or company engineering blog.
4. **Pain hypothesis**: The specific testing problem we hypothesized in the outreach.
5. **What triggered the reply**: From the reply tag (opener, pain hook, proof point, timing, etc.) - this tells Rob what resonated.
6. **Suggested discovery questions** (3-5, tailored):
   - "Walk me through how your team tests [specific product/workflow from research]."
   - "What's your current automation stack look like?" (if tech stack is unknown)
   - "Where are regression cycles hitting hardest?"
   - "Is there a timeline or leadership mandate driving this?"
   - One question specific to their industry/situation.
7. **Relevant proof points**: 2-3 customer stories matched to their vertical and pain, with specific numbers Rob can drop.
8. **Predicted objections**: The pre-mapped objection + response from the tracker.

**Rules:**
- The prep card is generated from data already in the tracker (research notes, reply tags, objection mapping). No new research needed.
- Keep it to one screen. Rob should be able to read it in 2 minutes before the call.
- Include a "Copy Prep" button so Rob can paste it into his CRM or notes.

### Qualification Checklist (per prospect)
- [ ] Manager+ seniority
- [ ] ICP title match
- [ ] No prior interaction in Sales Navigator
- [ ] US-based (unless specified otherwise)
- [ ] Software QA/engineering (not pharma manufacturing, biotech lab QA, etc.)
- [ ] Company has software products or digital platforms to test
- [ ] Profile URL captured and logged

### Common Pitfalls to Avoid
- **Don't send messages.** Rob will copy/paste manually.
- **Don't use the same customer proof point for every message.** Rotate and match to the prospect's pain.
- **Don't write "I noticed" or "I saw" as the opener for more than ~3 messages in a batch.** Vary it.
- **Don't include prospects from the same company more than twice** in a single batch (unless explicitly asked).
- **Don't include prospects whose titles are VP SE / VP Engineering at companies with 50K+ employees** unless there's a Buyer Intent signal or they specifically own QA. At massive orgs, these VPs are too far from the testing decision.
- **Do prioritize QA-titled leaders** (Director of QA, Head of QA, VP Quality Engineering) over generic VP Engineering titles. They feel the pain directly and have authority to evaluate.
- **Do flag Buyer Intent signals** from Sales Navigator prominently in the tracker.
- **Do use specific numbers** in proof points (not "significant reduction" but "50% reduction" or "3 days to 80 minutes").

→ Full outreach templates and examples: memory/context/sales-playbook.md

---

## Outbound Intelligence System (Data-Driven Rules - Feb 2026)

This section defines the CANONICAL rules derived from analysis of 1,330 LinkedIn conversations (384 replies, 946 no-replies, 6,210 messages). These rules override any generic best practices. When data conflicts with intuition, the data wins.

Source document: `linkedin-outreach-analysis.docx` and `analysis_output.json`

### Hard Constraints (MUST NEVER be violated)

Any draft message violating these is automatically rejected and rewritten.

| # | Constraint | Data Basis |
|---|-----------|------------|
| HC1 | No "reaching out" / "wanted to connect" / "I saw" / "I noticed" phrasing | -13.3 pp diff (14.7% reply vs 28.0% no-reply). Strongest negative signal. |
| HC2 | No role-at-company as the primary opener hook ("Seeing that you're the [Title] at [Company]") | -12.7 pp diff (86.1% vs 98.8%). Present in 98.8% of failures. |
| HC3 | No feature-led framing (AI, self-healing, automation as the headline) | ai_mention -8.0 pp, self_healing -6.8 pp. Features as headlines lose. |
| HC4 | No messages over 120 words | Reply avg 98.7 words vs no-reply 107.7 words. Shorter wins. |
| HC5 | No evening sends (after 6 PM) | Evening: 10.8% of replies vs 19.4% of no-replies. -8.6 pp swing. |
| HC6 | No bullet-point feature lists | Bullet-format messages dominate no-reply samples. 0 high-performers use bullets. |
| HC7 | No permission-based CTAs ("would it be unreasonable", "happy to share if helpful") | permission_ask CTA: 23.5% rate (below 27.5% baseline). Found in multiple no-reply samples. |

### Strong Preferences (optimize for)

| # | Preference | Data Basis |
|---|-----------|------------|
| SP1 | Under 100 words (~580 chars) | Reply avg: 578 chars / 98.7 words |
| SP2 | Afternoon send window (12-5 PM) | 88.7% of replies sent in afternoon vs 80.4% no-reply |
| SP3 | Question-led openers grounded in research | question_opener: +2.6 pp diff |
| SP4 | Exactly one outcome-based proof point | customer_story neutral (55.2% vs 55.7%) - relevance matters, not volume |
| SP5 | Direct, confident CTA with time/action | meeting_ask: 75.0% rate (directional, small n) |
| SP6 | Vary message angle across touches | High-performer threads: 4.7 avg msgs with diverse angles |
| SP7 | Commit to at least 3 touches before expecting reply | 28.2% of replies came after 2 msgs, 31.3% after 3+ |

### Message Quality Score (MQS) - 12-Point System

Every outbound message is scored on 4 dimensions (1-3 each). Maximum = 12.

**Dimension 1: Opener Clarity (1-3)**
- 3 = Specific, insight-driven question about the prospect's situation. No reaching_out, no role-recap.
- 2 = Company-specific reference but generic frame. May include mild role reference.
- 1 = Opens with "I'm reaching out" or "Seeing that you're the [Title]." Template-visible.

**Dimension 2: CTA Confidence (1-3)**
- 3 = Specific ask with time/action, tied to the value angle. No easy outs.
- 2 = Asks a question but doesn't specify time or tie to value angle.
- 1 = Permission-style, vague, or puts work on the prospect.

**Dimension 3: Personalization Density (1-3)**
- 3 = References something only THIS person would recognize (specific project, career move, company event).
- 2 = References their company specifically but opener could apply to anyone in that role.
- 1 = Only swaps name, title, company. Indistinguishable from a template.

**Dimension 4: Friction (1-3, inverted: 3=low friction)**
- 3 = Under 100 words, one proof point, one CTA, no bullet lists, clean and scannable.
- 2 = 100-120 words, may have 2 proof points, still readable.
- 1 = Over 120 words, bullet-point features, multiple CTAs, wall-of-text.

**Interpretation:**
- 10-12 = Ready to send
- 7-9 = Acceptable but should be improved
- <7 = MUST be rewritten before sending

### QA Gate (MANDATORY before any batch is presented)

1. **Rule violation scan**: Check every message against all 7 Hard Constraints. Any violation = auto-rewrite.
2. **Score computation**: Compute MQS for every message. Show breakdown.
3. **Threshold enforcement**: Only messages scoring >=9/12 may be presented.
4. **Structural dedup**: No two messages in the batch may be structurally identical.
5. **Evidence check**: Every personalization claim must have a research source.
6. **Angle rotation check**: No prospect may receive the same angle in more than one touch.

### Prospect Prioritization (Reply-Likelihood Model)

Prioritize based on "likelihood of reply given historical behavior," NOT theoretical ICP fit.

**Boost factors (from observed reply data):**
- QA-titled leaders (Director/Head/VP of QA): highest reply rate personas
- Companies where a specific, evidence-backed opener question can be formed (Personalization Score 3 potential)
- Companies in active transformation, acquisition, or scaling (gives natural opener)
- Buyer Intent signals from Sales Navigator

**Penalize factors (from observed no-reply data):**
- Prospects where only surface-level personalization is possible (name/title/company swaps = Score 1)
- VP Eng at 50K+ companies with no QA-specific scope
- Prospects requiring >120 words to explain the relevance (high friction risk)

### Proof Point Rotation Logic

Never use the same proof point twice for the same prospect across their sequence.

| Angle | Proof Points | Best Verticals |
|-------|-------------|----------------|
| Maintenance | Hansard 8wk→5wk, 90% maintenance reduction, 70% vs Selenium | Insurance, FinServ, long regression |
| Velocity | CRED 90% coverage + 5x faster, Sanofi 3 days→80 min | FinTech, fast-shipping teams |
| Scale/Coverage | Medibuddy 2,500 tests + 50% maintenance cut, Nagra DTV 2,500 in 8mo | Healthcare, media, mid-size scaling |
| Productivity | Fortune 100 3X productivity, Spendflo 50% manual cut | Enterprise VP-level, SaaS startups |

### Observed Reply Patterns (for response handling)

| Type | % of Replies | Action |
|------|-------------|--------|
| Polite ("thanks") | 37.9% | Follow up with value. Not commitment. |
| Positive ("interested") | 22.8% | Book meeting immediately. Don't over-explain. |
| Negative ("not interested") | 9.4% | Log objection. May re-engage 60+ days. |
| Curiosity ("how", "tell me more") | 8.3% | Answer directly, then bridge to meeting. |
| Referral ("talk to [name]") | 7.4% | High value. Reach out to referred person immediately. |
| Has tool ("we use [X]") | 2.3% | Objection handle: ask about gaps. |
| Timing ("not right now") | 2.1% | Set calendar reminder. Re-engage per triggers. |
